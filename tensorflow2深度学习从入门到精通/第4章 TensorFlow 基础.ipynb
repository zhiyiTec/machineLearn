{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============4.1.1 数值 数值 类型==============\n",
      "tf.Tensor(\n",
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]], shape=(2, 2, 2), dtype=int32)\n",
      "(2, 2, 2)\n",
      "==============4.1.2 字符串类型==============\n",
      "tf.Tensor(b'hello, deep learning.', shape=(), dtype=string)\n",
      "==============4.1.3 布尔类型==============\n",
      "tf.Tensor(True, shape=(), dtype=bool)\n",
      "tf.Tensor([ True False], shape=(2,), dtype=bool)\n",
      "tf.Tensor(True, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# 4.1 数据类型\n",
    "## 4.1.1 数值 数值 类型\n",
    "import tensorflow as tf\n",
    "print(\"==============4.1.1 数值 数值 类型==============\")\n",
    "a = tf.constant([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
    "print(a)\n",
    "print(a.shape)\n",
    "\n",
    "## 4.1.2 字符串类型\n",
    "print(\"==============4.1.2 字符串类型==============\")\n",
    "b = tf.constant('HELLO, Deep Learning.')# 实例化一个字符串变量\n",
    "b=tf.strings.lower(b) # 将所有字母变为小写\n",
    "print(b)\n",
    "## 4.1.3 布尔类型\n",
    "print(\"==============4.1.3 布尔类型==============\")\n",
    "c= tf.constant(True)\n",
    "print(c)\n",
    "c = tf.constant([True, False])\n",
    "print(c)\n",
    "### TensorFlow 的布尔类型和 Python 语言的布尔类型并不对等，不能通用\n",
    "c = tf.constant(True) # 创建布尔张量\n",
    "print(c==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-13035, shape=(), dtype=int16)\n",
      "tf.Tensor(123456789, shape=(), dtype=int32)\n",
      "tf.Tensor(3.1415927, shape=(), dtype=float32)\n",
      "tf.Tensor(3.141592653589793, shape=(), dtype=float64)\n",
      "before: <dtype: 'int16'>\n",
      "after : <dtype: 'float32'>\n",
      "a转double:<dtype: 'float64'>\n",
      "a有32转16位: tf.Tensor(-13035, shape=(), dtype=int16)\n",
      "布尔型与整形之间相互转换： tf.Tensor([1 0], shape=(2,), dtype=int32)\n",
      "整型转bool类型： tf.Tensor([ True False  True  True], shape=(4,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# 4.2 数值精度\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "a=tf.constant(123456789, dtype=tf.int16)\n",
    "a1=tf.constant(123456789, dtype=tf.int32)\n",
    "print(a)\n",
    "print(a1)\n",
    "a2=tf.constant(np.pi, dtype=tf.float32)\n",
    "a3=tf.constant(np.pi, dtype=tf.float64)\n",
    "print(a2)\n",
    "print(a3)\n",
    "## 4.2.1 读取精度\n",
    "print('before:',a.dtype)\n",
    "if a.dtype != tf.float32:\n",
    "    a = tf.cast(a,tf.float32) # 转换精度\n",
    "print('after :',a.dtype)\n",
    "## 4.2.2 类型转换\n",
    "a = tf.constant(np.pi, dtype=tf.float16)\n",
    "a=tf.cast(a, tf.double)\n",
    "print(\"a转double:{}\".format(a.dtype))\n",
    "### 进行类型转换时，需要保证转换操作的合法性，例如将高精度的张量转换为低精度的张量时，可能发生数据溢出隐患：\n",
    "a=tf.constant(123456789,dtype=tf.int32)\n",
    "a=tf.cast(a,tf.int16)\n",
    "print(\"a有32转16位:\",a)\n",
    "### 布尔型与整形之间相互转换也是合法的，是比较常见的操作：\n",
    "a = tf.constant([True, False])\n",
    "a=tf.cast(a,tf.int32)\n",
    "print(\"布尔型与整形之间相互转换：\",a)\n",
    "### 一般默认 0 表示 False，1 表示 True，在 TensorFlow 中，将非 0 数字都视为 True\n",
    "a = tf.constant([-1, 0, 1, 2])\n",
    "a=tf.cast(a, tf.bool)\n",
    "print(\"整型转bool类型：\",a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([-1,  0,  1,  2])>\n",
      "Variable:0\n",
      "True\n",
      "b <tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[1, 2],\n",
      "       [3, 4]])>\n"
     ]
    }
   ],
   "source": [
    "# 4.3 待优化张量\n",
    "### 张量的 name 和 trainable 属性是 Variable 特有的属性，\n",
    "### name 属性用于命名计算图中的变量，这套命名体系是 TensorFlow 内部维护的，一般不需要用户关注 name 属性；\n",
    "### trainable表征当前张量是否需要被优化，创建 Variable 对象是默认启用优化标志，可以设置trainable=False 来设置张量不需要优化。\n",
    "a=tf.constant([-1,0,1,2])\n",
    "aa=tf.Variable(a)\n",
    "print(aa)\n",
    "print(aa.name)\n",
    "print(aa.trainable)\n",
    "### 除了通过普通张量方式创建 Variable，也可以直接创建：\n",
    "b = tf.Variable([[1,2],[3,4]])\n",
    "print(\"b\",b)\n",
    "### 待优化张量可看做普通张量的特殊类型，普通张量也可以通过 GradientTape.watch()方法临时加入跟踪梯度信息的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tf.Tensor([1. 2.], shape=(2,), dtype=float32)\n",
      "aa: tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float64)\n",
      "b: tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "b1 tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "b2 tf.Tensor([1.], shape=(1,), dtype=float32)\n",
      "b3 tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "b4 tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(2, 3), dtype=float32)\n",
      "b5 tf.Tensor(\n",
      "[[1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]], shape=(6, 6), dtype=float32)\n",
      "b7 tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(2, 3), dtype=float32)\n",
      "C: tf.Tensor(\n",
      "[[-1 -1 -1 -1]\n",
      " [-1 -1 -1 -1]\n",
      " [-1 -1 -1 -1]], shape=(3, 4), dtype=int32)\n",
      "d: tf.Tensor(\n",
      "[[-1.1901563   0.21974714]\n",
      " [ 0.06932668  0.40315804]], shape=(2, 2), dtype=float32)\n",
      "d1 tf.Tensor(\n",
      "[[-0.10858321  0.12188458]\n",
      " [ 2.537197    4.51826   ]], shape=(2, 2), dtype=float32)\n",
      "d2 tf.Tensor(\n",
      "[[0.92485726 0.1894319 ]\n",
      " [0.724565   0.32994676]], shape=(2, 2), dtype=float32)\n",
      "d3 tf.Tensor(\n",
      "[[7.8135395 1.5787005]\n",
      " [4.4171677 6.9718194]], shape=(2, 2), dtype=float32)\n",
      "d4 tf.Tensor(\n",
      "[[87 75]\n",
      " [41 22]], shape=(2, 2), dtype=int32)\n",
      "e tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\n",
      "e1 tf.Tensor([0 2 4 6 8], shape=(5,), dtype=int32)\n",
      "e2 tf.Tensor([1 3 5 7 9], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 4.4 创建张量\n",
    "### Numpy Array 数组和 Python List 是 Python 程序中间非常重要的数据载体容器\n",
    "### 很多数据都是通过 Python 语言将数据加载至 Array 或者 List 容器，再转换到 Tensor 类型\n",
    "### 通过TensorFlow 运算处理后导出到 Array 或者 List 容器，方便其他模块调用。\n",
    "## 4.4.1 从 从 Numpy, List 对象 创建\n",
    "### 通过 tf.convert_to_tensor 可以创建新 Tensor，并将保存在 Python List 对象或者 NumpyArray 对象中的数据导入到新 Tensor 中\n",
    "a=tf.convert_to_tensor([1,2.])\n",
    "### 需要注意的是，Numpy 中浮点数数组默认使用 64-Bit 精度保存数据，\n",
    "### 转换到 Tensor 类型时精度为 tf.float64，可以在需要的时候转换为 tf.float32 类型。\n",
    "aa=tf.convert_to_tensor(np.array([[1,2.],[3,4]]))\n",
    "print(\"a:\",a)\n",
    "print(\"aa:\",aa)\n",
    "## 4.4.2 创建全 全 0 ，全 1 张量\n",
    "b=tf.zeros([])\n",
    "b1=tf.ones([])\n",
    "b2=tf.ones([1])\n",
    "b3=tf.zeros([1])\n",
    "print(\"b:\",b)\n",
    "print(\"b1\",b1)\n",
    "print(\"b2\",b2)\n",
    "print(\"b3\",b3)\n",
    "# 创建全 0 的矩阵：\n",
    "b4=tf.zeros([2,3])\n",
    "print(\"b4\",b4)\n",
    "# 创建全1矩阵\n",
    "b5=tf.ones([6,6])\n",
    "print(\"b5\",b5)\n",
    "### 过 tf.zeros_like, tf.ones_like 可以方便地新建与某个张量 shape 一致，内容全 0 或全 1的张量。\n",
    "### 例如，创建与张量 b6 形状一样的全 0 张量：\n",
    "b6=tf.ones([2,3])\n",
    "b7=tf.zeros_like(b6)\n",
    "print(\"b7\",b7)\n",
    "    ## 4.4.3 创建 创建 自定义数值\n",
    "### 除了初始化为全 0，或全 1 的张量之外，有时也需要全部初始化为某个自定义数值的张量，比如将张量的数值全部初始化为-1 等。\n",
    "### 通过 tf.fill(shape, value)可以创建全为自定义数值 value 的张量。例如，创建元素为-1的标量：\n",
    "c=tf.fill([3,4],-1)\n",
    "print(\"C:\",c)\n",
    "## 4.4.4 创建已知分布的张量\n",
    "### 正态分布(Normal Distribution，或 Gaussian Distribution)\n",
    "### 均匀分布(UniformDistribution)是最常见的分布之一，创建采样自这 2 种分布的张量非常有用\n",
    "### 比如在卷积神经网络中，卷积核张量 W 初始化为正态分布有利于网络的训练；\n",
    "### 在对抗生成网络中，隐藏变量 z 一般采样自均匀分布。\n",
    "### 通过 tf.random.normal(shape, mean=0.0, stddev=1.0)可以创建\n",
    "### 形状为 shape，均值为mean,标准差为 stddev 的正态分布𝒩(𝑛𝑓𝑏𝑜,𝑡𝑢𝑒𝑒𝑓𝑤 2 )。\n",
    "### 下面创建一个均值为0，标准差为1的正态分布\n",
    "d=tf.random.normal([2,2])\n",
    "print(\"d:\",d)\n",
    "### 创建均值为 1，标准差为 2 的正太分布：\n",
    "d1=tf.random.normal([2,2], mean=1,stddev=2)\n",
    "print(\"d1\",d1)\n",
    "### 通过 tf.random.uniform(shape, minval=0, maxval=None, dtype=tf.float32)可以创建采样自[𝑛𝑗𝑜𝑤𝑏𝑚,𝑛𝑏𝑦𝑤𝑏𝑚]区间的均匀分布的张量。\n",
    "### 例如创建采样自区间[0,1]，shape 为[2,2]的矩阵：\n",
    "d2=tf.random.uniform([2,2])\n",
    "print(\"d2\",d2)\n",
    "### 创建采样自区间[0,10]，shape 为[2,2]的矩阵：\n",
    "d3=tf.random.uniform([2,2],maxval=10)\n",
    "print(\"d3\",d3)\n",
    "### 如果需要均匀采样整形类型的数据，必须指定采样区间的最大值 maxval 参数，同时制定数据类型为 tf.int*型：\n",
    "d4=tf.random.uniform([2,2],maxval=100,dtype=tf.int32)\n",
    "print(\"d4\",d4)\n",
    "## 4.4.5 创建序列\n",
    "### 在循环计算或者对张量进行索引时，经常需要创建一段连续的整形序列\n",
    "### 可以通过tf.range()函数实现。\n",
    "### tf.range(limit, delta=1)可以创建[0,𝑚𝑗𝑛𝑗𝑢)之间，步长为 delta 的整形序列，不包含 limit 本身。例如，创建 0~9，步长为 1 的整形序列\n",
    "e=tf.range(10,delta=1)\n",
    "print(\"e\",e)\n",
    "### 创建 0~9，步长为 2 的整形序列：\n",
    "e1=tf.range(10,delta=2)\n",
    "print(\"e1\",e1)\n",
    "### tf.range(start, limit, delta=1)可以创建[𝑡𝑢𝑏𝑠𝑢,𝑚𝑗𝑛𝑗𝑢)，步长为 delta 的序列，不包含 limit本身：\n",
    "e2=tf.range(1,limit=10,delta=1)\n",
    "print(\"e2\",e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
